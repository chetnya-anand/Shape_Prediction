{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 840 images belonging to 3 classes.\n",
      "Found 360 images belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 41s 1s/step - loss: 0.7396 - acc: 0.6321 - val_loss: 0.3950 - val_acc: 0.7778\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 40s 1s/step - loss: 0.2431 - acc: 0.8917 - val_loss: 0.0820 - val_acc: 0.9611\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 40s 1s/step - loss: 0.0419 - acc: 0.9857 - val_loss: 0.0273 - val_acc: 0.9889\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 41s 1s/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 40s 1s/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from skimage.measure import block_reduce\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageFilter\n",
    "import tensorflow\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_folder = 'shapes'\n",
    "img_size = (128, 128)\n",
    "batch_size = 30\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n",
    "                             validation_split=0.3) # hold back 30% of the images for validation\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_folder,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_folder,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "classnames = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Define the model as a sequence of layers\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer accepts an image and applies a convolution that uses 32 6x6 filters and a rectified linear unit activation function\n",
    "model.add(Conv2D(32, (6, 6), input_shape=train_generator.image_shape, activation='relu'))\n",
    "\n",
    "# Next we;ll add a max pooling layer with a 2x2 patch\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We can add as many layers as we think necessary - here we'll add another convolution, max pooling, and dropout layer\n",
    "model.add(Conv2D(32, (6, 6), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# And another set\n",
    "model.add(Conv2D(32, (6, 6), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# A dropout layer randomly drops some nodes to reduce inter-dependencies (which can cause over-fitting)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Now we'll flatten the feature maps and generate an output layer with a predicted probability for each class\n",
    "model.add(Flatten())\n",
    "model.add(Dense(train_generator.num_classes, activation='sigmoid'))\n",
    "\n",
    "# With the layers defined, we can now compile the model for categorical (multi-class) classification\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model over 3 epochs using 30-image batches and using the validation holdout dataset for validation\n",
    "num_epochs = 5\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = num_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
